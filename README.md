# GrabLang ğŸ•·ï¸ - Le PostgreSQL du Web Scraping

**GrabLang** est un langage de domaine spÃ©cifique (DSL) conÃ§u pour le web scraping avancÃ©. InspirÃ© par la puissance et l'expressivitÃ© de PostgreSQL, GrabLang offre une syntaxe simple mais extrÃªmement puissante pour extraire, transformer et analyser des donnÃ©es web.

## ğŸ¯ Vision

CrÃ©er un langage de scraping aussi expressif et puissant que PostgreSQL l'est pour les bases de donnÃ©es :
- **RequÃªtes dÃ©claratives** : DÃ©crivez ce que vous voulez, pas comment l'obtenir
- **Pipeline de transformation** : ChaÃ®nez les opÃ©rations comme des requÃªtes SQL
- **Typage intelligent** : DÃ©tection automatique des types de donnÃ©es
- **Optimisation automatique** : Le moteur optimise vos requÃªtes de scraping

## ğŸš€ FonctionnalitÃ©s Actuelles

### ğŸ“¥ Chargement de DonnÃ©es (LOAD)
```grab
LOAD URL "https://example.com"
LOAD FILE "page.html"
LOAD JSON "data.json"
```

### ğŸ¯ SÃ©lection d'Ã‰lÃ©ments (SELECT)
```grab
SELECT ALL "div.article"           # Tous les articles
SELECT FIRST "h1"                  # Premier titre
SELECT LAST "img"                  # DerniÃ¨re image
SELECT ONCE "a" 3                  # TroisiÃ¨me lien (index 1-based)
```

### ğŸ” Extraction de DonnÃ©es (GET)
```grab
# Extraction d'attributs
GET ATTR "href"                    # Attributs des Ã©lÃ©ments sÃ©lectionnÃ©s
GET ATTR FIRST "a" "href"          # URL du premier lien
GET ATTR LAST "img" "src"          # Source de la derniÃ¨re image
GET ATTR ONCE "button" 2 "class"   # Classes du 2Ã¨me bouton

# DÃ©tection intelligente de dates
GET DATE "article"                 # Articles contenant des dates
GET DATE FIRST "span"              # Premier span avec une date
GET DATE LAST "time"               # Dernier Ã©lÃ©ment time avec date
GET DATE ONCE "p" 1                # VÃ©rifie si le 1er paragraphe a une date
```

### ğŸ’¾ Gestion des Variables (SAVE/USE)
```grab
SELECT ALL "a"
GET ATTR "href"
SAVE all_links                     # Sauvegarde dans une variable

USE all_links                      # RÃ©utilise la variable
FILTER CONTAINS "github"           # Filtre les liens GitHub
SAVE github_links
```

### ğŸ¨ SystÃ¨me de Debug ColorÃ©
- **Mode debug** avec couleurs distinctives pour chaque type de commande
- **TraÃ§abilitÃ© complÃ¨te** de l'exÃ©cution
- **Messages d'erreur** prÃ©cis et contextuels

## ğŸ›£ï¸ Roadmap - Vers la Puissance PostgreSQL

### ğŸ”„ Phase 1 : Transformations (En cours)
```grab
# Filtrage avancÃ©
FILTER WHERE class CONTAINS "active"
FILTER WHERE text MATCHES "^[0-9]+$"
FILTER WHERE attr href NOT NULL

# Extraction de texte
EXTRACT TEXT                       # Contenu textuel
EXTRACT TEXT CLEAN                 # Texte nettoyÃ© (sans espaces extra)
EXTRACT NUMBERS                    # Extraction de nombres
EXTRACT EMAILS                     # Extraction d'emails
EXTRACT URLS                       # Extraction d'URLs dans le texte
```

### ğŸ“Š Phase 2 : AgrÃ©gations et Analyses
```grab
# Fonctions d'agrÃ©gation (comme SQL)
COUNT                              # Nombre d'Ã©lÃ©ments
COUNT DISTINCT attr href           # URLs uniques
SUM NUMBERS                        # Somme des nombres trouvÃ©s
AVG NUMBERS                        # Moyenne des nombres
GROUP BY attr class                # Groupement par classes CSS

# Statistiques avancÃ©es
STATS TEXT LENGTH                  # Statistiques de longueur de texte
STATS LINKS                        # Analyse des liens (domaines, types)
STATS IMAGES                       # Analyse des images (formats, tailles)
```

### ğŸ”— Phase 3 : Relations et Jointures
```grab
# Relations entre Ã©lÃ©ments (comme JOIN)
RELATE PARENT "div"                # Ã‰lÃ©ments parents
RELATE CHILDREN "span"             # Ã‰lÃ©ments enfants
RELATE SIBLINGS "li"               # Ã‰lÃ©ments frÃ¨res

# Jointures avec donnÃ©es externes
JOIN WITH FILE "urls.csv" ON href
JOIN WITH API "https://api.example.com" ON id
```

### ğŸ§  Phase 4 : Intelligence Artificielle
```grab
# Classification automatique
CLASSIFY SENTIMENT                 # Analyse de sentiment
CLASSIFY CATEGORY                  # CatÃ©gorisation automatique
CLASSIFY LANGUAGE                  # DÃ©tection de langue

# Extraction sÃ©mantique
EXTRACT ENTITIES                   # EntitÃ©s nommÃ©es (personnes, lieux)
EXTRACT KEYWORDS                   # Mots-clÃ©s importants
EXTRACT SUMMARY                    # RÃ©sumÃ© automatique
```

### ğŸ“ˆ Phase 5 : Optimisation et Performance
```grab
# Cache intelligent
CACHE STRATEGY LRU                 # StratÃ©gie de cache
CACHE TTL 3600                     # Time-to-live

# ParallÃ©lisation
PARALLEL WORKERS 4                 # Scraping parallÃ¨le
BATCH SIZE 100                     # Traitement par lots

# Monitoring
MONITOR PERFORMANCE                # MÃ©triques de performance
MONITOR ERRORS                     # Suivi des erreurs
```

### ğŸ—ï¸ Phase 6 : Fonctions et ProcÃ©dures
```grab
# Fonctions personnalisÃ©es
FUNCTION extract_price(text)
    EXTRACT NUMBERS
    FILTER WHERE value > 0
    RETURN FIRST
END

# ProcÃ©dures stockÃ©es
PROCEDURE scrape_ecommerce(url)
    LOAD URL url
    SELECT ALL ".product"
    EXTRACT price USING extract_price(text)
    SAVE products
END
```

### ğŸ“¡ Phase 7 : IntÃ©grations et APIs
```grab
# IntÃ©grations bases de donnÃ©es
OUTPUT TO POSTGRES "postgresql://..."
OUTPUT TO MONGODB "mongodb://..."
OUTPUT TO ELASTICSEARCH "http://..."

# APIs et webhooks
POST TO API "https://api.example.com/data"
WEBHOOK ON ERROR "https://alerts.example.com"

# Formats d'export
EXPORT AS JSON "data.json"
EXPORT AS CSV "data.csv"
EXPORT AS PARQUET "data.parquet"
```

## ğŸ›ï¸ Architecture

### ğŸ§© SystÃ¨me Modulaire
- **Handlers dynamiques** : Chaque commande est un module indÃ©pendant
- **Chargement automatique** : DÃ©couverte automatique des commandes
- **ExtensibilitÃ©** : Ajout facile de nouvelles fonctionnalitÃ©s

### ğŸ¨ SystÃ¨me de Types
- **Types primitifs** : string, number, boolean, date
- **Types complexes** : Element, ElementList, URL, Email
- **Types composÃ©s** : Table, Graph, Tree
- **InfÃ©rence de types** : DÃ©tection automatique

### âš¡ Moteur d'Optimisation
- **Plan d'exÃ©cution** : Optimisation des requÃªtes complexes
- **Cache multi-niveaux** : DOM, HTTP, rÃ©sultats
- **ParallÃ©lisation intelligente** : Optimisation automatique

## ğŸ’¡ Exemples d'Usage

### ğŸ¢ Scraping E-commerce
```grab
LOAD URL "https://shop.example.com/products"
SELECT ALL ".product-card"
GET ATTR "data-product-id"
SAVE product_ids

USE product_ids
FOREACH id
    LOAD URL "https://shop.example.com/product/" + id
    EXTRACT price FROM ".price"
    EXTRACT title FROM "h1"
    EXTRACT rating FROM ".stars"
    SAVE TO products_table
END
```

### ğŸ“° Analyse de News
```grab
LOAD URL "https://news.example.com"
SELECT ALL "article"
FILTER WHERE GET DATE NOT NULL
GET DATE FIRST "time"
EXTRACT headline FROM "h2"
EXTRACT summary FROM ".excerpt"
CLASSIFY SENTIMENT ON summary
GROUP BY sentiment
EXPORT AS JSON "news_analysis.json"
```

### ğŸ” Monitoring de CompÃ©tition
```grab
PROCEDURE monitor_competitor(url)
    LOAD URL url
    SELECT ALL ".product"
    EXTRACT price USING extract_price(text)
    COMPARE WITH PREVIOUS_RUN
    IF price_change > 5%
        ALERT "Price change detected: " + price_change
    END
END

SCHEDULE DAILY monitor_competitor("https://competitor.com")
```

## ğŸ› ï¸ Installation et Usage

```bash
# Installation
pip install -r requirements.txt

# ExÃ©cution d'un script
python src/core/interpreter.py script/example.grab

# Mode debug
python src/core/interpreter.py script/example.grab --debug
```

## ğŸ¯ Objectifs Ã  Long Terme

1. **ExpressivitÃ© PostgreSQL** : Toute la puissance des requÃªtes SQL pour le web
2. **Performance** : Scraping Ã  grande Ã©chelle avec optimisations automatiques
3. **Intelligence** : IA intÃ©grÃ©e pour la comprÃ©hension sÃ©mantique
4. **Ã‰cosystÃ¨me** : BibliothÃ¨que de fonctions et connecteurs
5. **Standards** : Format d'Ã©change inter-outils de scraping

## ğŸ¤ Contribution

GrabLang est conÃ§u pour Ãªtre extensible. Chaque nouvelle fonctionnalitÃ© suit l'architecture modulaire :

1. **CrÃ©er un handler** dans `src/commands/`
2. **ImplÃ©menter les sous-commandes** 
3. **Ajouter les tests** correspondants
4. **Documenter** la nouvelle fonctionnalitÃ©

## ğŸ“œ Licence

[Votre licence ici]

---

**GrabLang** - *"Query the web like a database"* ğŸŒğŸ’«